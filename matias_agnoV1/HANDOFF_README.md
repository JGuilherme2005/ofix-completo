# üìò Manual de Handoff - Projeto Matias & Ofix (Integra√ß√£o IA)

Este documento serve como guia t√©cnico para continuar o desenvolvimento do assistente **Matias** (IA da Ofix) e sua integra√ß√£o com o sistema principal.

---

## üèóÔ∏è 1. Vis√£o Geral da Arquitetura

O sistema opera em uma arquitetura h√≠brida:

1.  **Frontend/Backend Ofix (Node.js/React)**:
    *   Gerencia a interface do usu√°rio (Chat, Dashboard).
    *   Recebe mensagens do usu√°rio e decide se processa localmente ou envia para a IA.
    *   Porta Padr√£o: `3001` (Backend), `3000` (Frontend).

2.  **Agente Matias (Python/Agno/FastAPI)**:
    *   "C√©rebro" da IA. Processa mensagens complexas usando LLMs.
    *   Suporta **Ollama** (local/remoto) e **Agno AI** (nuvem).
    *   Porta Padr√£o: `8001`.

### üîÑ Fluxo de Comunica√ß√£o
1.  Usu√°rio envia mensagem no Chat Ofix.
2.  Backend Node recebe em `/routes/agno.routes.js`.
3.  Node envia requisi√ß√£o POST para Python em `http://localhost:8001/agno/chat-inteligente`.
4.  Agente Python (`matias_agno/main.py` -> `agents/matias_ollama.py`) processa usando Ollama.
5.  Resposta √© devolvida ao Node e exibida no Chat.

---

## üöÄ 2. Como Rodar o Projeto

### Pr√©-requisitos
*   Python 3.10+
*   Node.js 18+
*   Ollama (rodando localmente ou endere√ßo remoto configurado).

### Passo 1: Iniciar o Agente (Python)
No diret√≥rio `matias_agnoV1`:

1.  Configure o `.env` (se necess√°rio):
    ```ini
    OLLAMA_ENABLED=true
    OLLAMA_BASE_URL=https://seu-ollama-url.com (ou http://localhost:11434)
    PORT=8001
    ```
2.  Inicie o servidor:
    ```bash
    python -m matias_agno.main
    ```
    *Dever√° aparecer: `‚úÖ Servidor rodando na porta 8001`.*

### Passo 2: Iniciar o Backend Ofix (Node.js)
No diret√≥rio `ofix-backend`:

1.  Certifique-se que o `.env` aponta para o agente local:
    ```ini
    AGNO_API_URL=http://localhost:8001
    ```
2.  Rode o servidor:
    ```bash
    npm run dev
    ```

---

## üìÇ 3. Arquivos Principais

### No Reposit√≥rio `matias_agnoV1` (Python)

*   **`matias_agno/agents/matias_ollama.py`**:
    *   Defini√ß√£o principal do agente. Configura o modelo (Ollama), instru√ß√µes (Prompt) e ferramentas.
    *   **Ponto de aten√ß√£o**: Se quiser mudar o comportamento da IA, edite as `instructions` aqui.

*   **`matias_agno/api.py`**:
    *   Define os endpoints da API (`/agno/chat-inteligente`).
    *   Define o contrato de dados (`ChatRequest`, `ChatResponse`).

*   **`matias_agno/main.py`**:
    *   Entrypoint. Decide se carrega o agente HuggingFace ou Ollama baseado no `.env`.
    *   Configura CORS (importante para o Frontend acessar).

### No Reposit√≥rio `OfixNovo` (Node.js)

*   **`ofix-backend/src/routes/agno.routes.js`**:
    *   Gerencia o roteamento de mensagens.
    *   Rota `/chat` (Node) -> chama `/agno/chat-inteligente` (Python).
    *   Implementa fallback (se Python cair, responde localmente).

---

## üõ†Ô∏è 4. Estado Atual e √öltimas Altera√ß√µes (Dez/2025)

*   **‚úÖ Integra√ß√£o Ollama Remoto**: O sistema est√° configurado para usar um servidor Ollama externo via Ngrok (ou local).
*   **‚úÖ Corre√ß√£o de Portas**: Padronizamos a comunica√ß√£o na porta `8001` para evitar conflitos com outros servi√ßos.
*   **‚úÖ API Schema**: Atualizamos o `matias_agno/api.py` para aceitar `session_id` e `user_id`, permitindo mem√≥ria persistente por usu√°rio.

---

## ‚ùì 5. Troubleshooting (Solu√ß√£o de Problemas)

### Erro: "429 Too Many Requests" ou "Connection Failed"
*   **Causa**: O Backend Node est√° tentando conectar na porta errada (8000) ou o servidor Python n√£o est√° rodando.
*   **Solu√ß√£o**: Verifique se `matias_agno/main.py` est√° rodando e se `AGNO_API_URL` no Node √© `http://localhost:8001`.

### Erro: "404 Not Found" ao acessar `/sessions/...` (No Agno OS UI)
*   **Causa**: Interface UI tentando acessar uma sess√£o antiga no banco de dados SQLite.
*   **Solu√ß√£o**: Limpe a URL do navegador (remova `?session=...`) ou clique em "New Chat".

### IA Respondendo "N√£o tenho acesso a essa informa√ß√£o"
*   **Causa**: Prompt do sistema muito restritivo ou falta de Knowledge Base.
*   **Solu√ß√£o**: Edite `matias_agno/agents/matias_ollama.py` e adicione instru√ß√µes/conhecimento.

---

## ‚òÅÔ∏è 6. Deploy no Render

O projeto est√° configurado para deploy via Docker ou Python nativo no **Render.com**.

### Configura√ß√£o do Servi√ßo (Web Service)
*   **Build Command**: `pip install -r matias_agno/requirements.txt`
*   **Start Command**: `python -m matias_agno.main`
*   **Porta**: O Render injeta a vari√°vel `PORT` automaticamente. O c√≥digo j√° est√° preparado para ler isso.

### Vari√°veis de Ambiente (Environment Variables) no Render:
Para funcionar com o **Ollama Remoto** (via Ngrok ou IP p√∫blico), voc√™ deve configurar:

| Vari√°vel | Valor Exemplo | Descri√ß√£o |
| :--- | :--- | :--- |
| `OLLAMA_ENABLED` | `true` | Ativa o agente Ollama em vez do HuggingFace |
| `OLLAMA_BASE_URL` | `https://seu-ollama.ngrok-free.app` | URL p√∫blica do seu servidor Ollama |
| `SUPABASE_DB_URL` | `postgresql://...` | (Opcional) Para persist√™ncia no Supabase |

> **‚ö†Ô∏è Aten√ß√£o:** Se o Render n√£o conseguir acessar seu Ollama (ex: se estiver no localhost sem t√∫nel), o agente falhar√° ao iniciar. Garanta que a URL do Ollama seja acess√≠vel publicamente.

---

## üîÆ 7. Pr√≥ximos Passos Sugeridos

1.  **Melhorar a Mem√≥ria**: Atualmente usa SQLite/Postgres. Verificar se a persist√™ncia de longo prazo est√° ideal para m√∫ltiplos usu√°rios.
2.  **Tools Customizadas**: Adicionar ferramentas Python para o agente consultar o banco de dados do Ofix diretamente (via Prisma ou SQL), permitindo que ele responda "Quanto faturamos hoje?" com dados reais.
3.  **Interface de Admin**: Criar uma tela no Ofix para configurar o Prompt do Matias sem precisar mexer no c√≥digo.

---
*Gerado por Agente Antigravity - 18/12/2025*
